{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Einführung in die Neuroinformatik - Blatt 03**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gruppe AC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1 - *Lernregeln*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Fehlerfunktion ist gegeben durch  \n",
    "> $E(w,b) = \\frac{1}{2} \\sum_{\\mu = 1}^M (T_{\\mu}-y(x_{\\mu}))^2  = \\frac{1}{2} \\sum_{\\mu = 1}^M (T_{\\mu}-f(wx_{\\mu}+b)^2$  \n",
    "\n",
    "mit der Netzausgabe $y(x_{\\mu})$, dem Lehrersignal $T_{\\mu}$, einem Gewicht $w$ und dem Bias $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Der Gradient der Fehlerfunktion $\\nabla E(w,b) = \\left(\\begin{array}{c} \\frac{\\partial E}{\\partial w}\\\\ \\frac{\\partial E}{\\partial b}\\\\ \\end{array} \\right)$ ist gegeben durch:  \n",
    "> $\\frac{\\partial E}{\\partial w} = \\frac{1}{2} \\sum_{\\mu = 1}^M 2 ((T_{\\mu}-y(x_{\\mu})) (-f'(wx_{\\mu}+b)) x_{\\mu} = - \\sum_{\\mu = 1}^M ((T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b) x_{\\mu}$\n",
    "> \n",
    "> $\\frac{\\partial E}{\\partial b} = \\frac{1}{2} \\sum_{\\mu = 1}^M 2 ((T_{\\mu}-y(x_{\\mu})) (-f'(wx_{\\mu}+b)) = - \\sum_{\\mu = 1}^M ((T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Die Lernregeln für das Gewicht $w$ und den Bias $b$ mit einer Lernrate $\\eta > 0$ lauten folgendermaßen:  \n",
    "a) Inkrementelle Version\n",
    "> $w(t+1) = w(t) + \\eta (T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b)x_{\\mu}$  \n",
    "> $b(t+1) = b(t) + \\eta (T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b)$  \n",
    "\n",
    "b) Batch - Version\n",
    "> $w(t+1) = w(t) + \\eta \\sum_{\\mu = 1}^M (T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b)x_{\\mu}$  \n",
    "> $b(t+1) = b(t) + \\eta \\sum_{\\mu = 1}^M (T_{\\mu}-y(x_{\\mu})) f'(wx_{\\mu}+b)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Datensatz initialisieren\n",
    "x = [-1, 0, 1, 2]\n",
    "T = [0, 1, 0, 0]\n",
    "\n",
    "#Gewicht und Bias initialisieren\n",
    "w0 = -1\n",
    "b0 = 3\n",
    "\n",
    "#Funktion f beschreiben\n",
    "def f(x):\n",
    "    result = 1/(1 + np.exp(-x))\n",
    "    return result\n",
    "\n",
    "#Ableitung von f beschreiben\n",
    "def df(x):\n",
    "    result = np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Berechnung des Gradienten an der Startposition $\\nabla E(w(0), b(0))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w0 = 0\n",
    "grad_b0 = 0\n",
    "for mu in range(0, len(x)):\n",
    "    grad_w0 -= (T[mu] - f(w0 * x[mu] + b0)) * df(w0 * x[mu] + b0) * x[mu]\n",
    "    grad_b0 -= (T[mu] - f(w0 * x[mu] + b0)) * df(w0 * x[mu] + b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) ![title](gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Ausführung einer Iteration der Batch-Modus-Lernregel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "\n",
    "w1 = w0\n",
    "b1 = b0\n",
    "for mu in range(0, len(x)):\n",
    "    w1 += eta * (T[mu] - f(w0 * x[mu] + b0)) * df(w0 * x[mu] + b0) * x[mu]\n",
    "    b1 += eta * (T[mu] - f(w0 * x[mu] + b0)) * df(w0 * x[mu] + b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Das Problem ist, dass das Gradientenverfahren ein lokales und kein globales Verfahren ist. Somit kann auch nur ein lokales Minimum gefunden werden. In diesem Fall wird ein lokales Minimum gefunden, das nicht dem globalen gesuchten Minimum entspricht."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
